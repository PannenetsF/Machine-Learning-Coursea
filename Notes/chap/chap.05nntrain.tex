\ifx\mainclass\undefined
\documentclass[en,11pt,english,black,simple]{../elegantbook}
\input{../needed.tex}
\begin{document}
\fi 
\def\chapname{05nntrain}

% Start Here
\chapter{Neural Network: Learning}

\section{Cost Function and Backpropagation}

\subsection{Cost Function}

Let's define symbols for n-class classification:

\begin{itemize}
    \item the input feature and its class: \({(x^{(1)}), y^{(1)}},{(x^{(2)}), y^{(2)}},\cdots,{(x^{(n)}), y^{(n)}}\)
    \item \(L\) is total number of layers
    \item \(s_l\) is the number of units in layer \(l\)
\end{itemize}

For logistic regression, the cost function is 


\[J(\theta) = \frac{1}{2m}\left[\sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})^2 + \lambda \sum_{i=1}^n\theta_i^2 \right]\]

For a neural network, it's:

\[\begin{aligned}
    J(\Theta)&=-\frac{1}{m}\left[\sum_{i=1}^{m} \sum_{k=1}^{K} y_{k}^{(i)} \log \left(h_{\Theta}\left(x^{(i)}\right)\right)_{k}+\left(1-y_{k}^{(i)}\right) \log \left(1-\left(h_{\Theta}\left(x^{(i)}\right)\right)_{k}\right)\right] \\
    &+\frac{\lambda}{2 m} \sum_{l=1}^{L-1} \sum_{i=1}^{s_{l}} \sum_{j=1}^{s_{l+1}}\left(\Theta_{j i}^{(l)}\right)^{2}\\
    \text{where: }& \quad \quad h_{\Theta}(x) \in \mathbb{R}^{K}, \quad\left(h_{\Theta}(x)\right)_{i}=i^{t h} \text { output } \\
\end{aligned}\]

The first term is for all \(K\) dimension output, and the last is the regular term of all weight in the neural network.

\subsection{Backpropagation Algorithm}

Backpropagation algorithm is a way to minimize the cost.

To use gradient descent, we need \(J(\theta)\)
 and \(\dfrac{\pp{J(\theta)}}{\pp{\Theta_{i,j}^{(l)}}}\).

So we have to compute the partical terms. We define the error of the \(L\) layer's node \(j\):

\[\delta_j^{(l)} = a^{(l)}_j - y_j\]

Then, for earlier layers:


\[\delta^{(l-1)} = (\Theta^{(l-1)})^T \delta^{(l)} .* g'(z^{(l-1)})\]

Where: 

\[g'(z^{(l)}) = a^{(l)} .* (1-a^{(l)})\]

Finally:

\[\frac{\pp{}}{\pp{\Theta_{i,j}^{(l)}}} J(\Theta) = a_j^{(l)} \delta_i^{(l+1)}, \text{when } \lambda = 0\]

For a training set  \({(x^{(1)}), y^{(1)}},{(x^{(2)}), y^{(2)}},\cdots,{(x^{(m)}), y^{(m)}}\):

\begin{lstlisting}
    Delta(l)(i,j) = 0
    for i = 1 : m
        set a(1) = x(i)
        compute for a(l) for l = 2,3,..,L
        with y(i), compute delta(L)
        then compute delta(L-1), ...,delta(1)
        Delta(l)(i,j) = Delta(l)(i,j) + a(l)(j) * delta(l+1)(i)
    endfor
\end{lstlisting}

Or in vector form: 

\[\Delta^{(l)} := \Delta^{(l)} + \delta^{(l+1)} (a^{(l)})^T\]



Then:

\[
\begin{aligned}
    D_{i,j}^{(l)} &= \frac{1}{m} \Delta_{i,j}^{(l)} + \lambda \Theta_{i,j}^{(i)}, \text{ if } j \neq 0\\
    D_{i,j}^{(l)} &= \frac{1}{m} \Delta_{i,j}^{(l)} + \lambda \Theta_{i,j}^{(i)}, \text{ if } j = 0
\end{aligned}
\]

And: 

\[\\
\dfrac{\pp{}}{\pp{\Theta_{i,j}^{(l)}}}J(\Theta) 1= D_{i,j} ^{(l)}\]

\subsection{Backpropagation Intuition}

Backpropagation is more likely to be a blankbox than previous algorithm.  

What do forward propagation do in the NNs? Doing non-linear matrix multiplication by introducing bias. And similarly, the former error is due to latter layers. 

\section{Backpropagation in Practice}

\subsection{Advanced Optimization}

Learn to unroll parameters matrices into vectors. 

\begin{lstlisting}
thetaVec = [Theta1(:); Theta2(:)];
\end{lstlisting}

The unrolled weights could be passed into \lstinline{fminunc(@cost, initTheta, option)}, where the \lstinline{cost} takes the unrolled vectors.

\subsection{Gradient Checking}

There could be some subtile bugs, so we need to check gradient.

We need to numerially compute the  gradient at a point, and it just needs 

\[\frac{\dd{}}{\dd{\theta}}J(\theta) \approx \frac{J(\theta + \epsilon) - J(\theta - \epsilon)}{2 \epsilon}\]

As for every parameter in the big vector, we can apply this by treat others as constant. Then we need to check that \(gradApprox \approx DVec\). Besides remember to disable check when you are training. 

\subsection{Random Initialization}

For gradient descent and other advanced methods we need to initialize all weight. If we set all weights to zero, the gradient will be all the same for each value will be zero and all weights in the same layer are in the identical.

To get through this, random initialization is applied. The key idea is to break symmetry. 

\subsection{Putting It Together}

Let's do some overall summary of neural network.

First we need a neural network architecture (a connectivity pattern between neurons). The number of input units is the dimension of features. The number of output units is the number of classes.

Then we need randomly initialize weights and implement the forward propagation. compute the cost function and do back propagation for partial derivatives. Of course we need to compute the gradient checking and after that remember to disable it. Finally, use something to compute.



\section{Application of Neural Network: Autonomous Driving}

\section*{Word}

subtile 微妙的

% End Here

\let\chapname\undefined
\ifx\mainclass\undefined
\end{document}
\fi 