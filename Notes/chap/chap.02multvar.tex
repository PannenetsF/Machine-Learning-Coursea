\ifx\mainclass\undefined
\documentclass[en,11pt,english,black,simple]{../elegantbook}
\input{../needed.tex}
\begin{document}
\fi 
\def\chapname{02multvar}

% Start Here
\chapter{Multivariate Linear Regression and Octave Tutorial}

\section{Multivariate Linear Regression}

With \textbf{Multivariate Linear Regression}, we can do predictions with more infomation. It appears that the expression will get more inputs or features.


\begin{note}
    \(x_j^{(i)}\) refers to the \(i^{th}\) training example's \(j_{th}\) feature value.
\end{note}

For example, for a n-varible hypothesis, its math form should be like:

\[
h_\theta(x) = \theta_0 + \theta_1 x_1 + \cdots + \theta_n x_n    
\]

Its vector form: 

\[
    x = \left[\begin{tabular}{c}
        \(x_0\)\\
        \(x_1\)\\
        \(\vdots\)\\
        \(x_n\)
    \end{tabular}\right] \in \mathbb{R}^{n+1}, 
        \theta = \left[\begin{tabular}{c}
            \(\theta_0\)\\
            \(\theta_1\)\\
            \(\vdots\)\\
            \(\theta_n\)
        \end{tabular}\right] \in \mathbb{R}^{n+1},
        \text{where } x_0 = 1 
\]

\[h_\theta(x) = \theta_0 + \theta_1 x_1 + \cdots + \theta_n x_n     = \theta^T\]



\section{Gradient Descent for Multiple Varibles}

For n-varible hypothesis, the cost function expresses like:

\[J(\theta) = \frac{1}{2m}\sum_{i=1}^m(h_\theta(x^(i)) - y^{(i)})^2\]

So the gradient descent performs like:

\[\begin{aligned}
    \theta_j := \theta_j - \alpha \frac{1}{m} \sum_{i = 0}^m (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}, \text{where } \theta = 0, 1, 2, \cdots , n
\end{aligned}\]

\section{Gradient Descent in Practice}

After the basic knowledge introduced, we are going to learn something in practice.

\subsection{Feature Scaling}

Make sure features are on a similar scale and the cost function can converge more quickly. If \(x_1 \in (0, 2000)\) and \(x_2 \in (1, 5)\), the step of gradient descent may fit with \(x_2\) but be too small to quickly converge.

A typical and useful opration is to scale feature into \((0, 1)\). More generally, get every feature into approximately a \([-1, 1]\) range. 

Or take mean normalization. Replace \(x_i\) with \(x_i - \mu_i\) or \({(x_i - \mu_i)}/{(\max(x)-\min(x))}\) to make features have approximately zero mean. 

\subsection{Learning Rate}

Make sure that gradient descent is working correctly, that is \(J(\theta)\) should decrease after each iteration. 

We can do automatic convergence test that if the decrease of \(J(\theta)\)  is less that a threshold, we declare the convergence.

If the \(\alpha\) is  too large, the loss may not converge or even diverge. If too small, it will take too long time to end the task.

\section{Features and Polynomial Regression}

Sometimes a straight line model would not fit a curve well, so that we choose polynomial regression to fit it with a polynomial.

For example, a 3-order polynomial like \(\theta_0 + \theta_1 x + \theta_2 x^2 + \theta_3 x^3\), we can do mapping like 

\[x \rightarrow x_1, x^2 \rightarrow x_2, x^3 \rightarrow x_3\]

Of course, in this example it's important to do scaling.

\section{Computing Parameters Analytically}

\subsection{Normal Equation}

Normal equantion is a method to solve for \(\theta\) analytically.

Using calculus we can solve for the global optimal for equation (single or multiple varibles).

Or, we can display our data in matrix form, then use the least square method.

For features, use \(X\); for outputs, use \(Y\):

\[
    X = \left[\begin{tabular}{c}
        \((x^{(0)})^T\)\\
        \((x^{(1)})^T\)\\
        \(\vdots\)\\
        \((x^{(n)})^T\)\\
    \end{tabular}\right] \in \mathbb{R}^{n+1},
    Y = \left[\begin{tabular}{c}
        \(y^{(0)}\)\\
        \(y^{(1)}\)\\
        \(\vdots\)\\
        \(y^{(n)}\)\\
    \end{tabular}\right] \in \mathbb{R}^{n+1}
\]

With least square method, get \(\Theta\) like:

\[\Theta = (X^T X)^{-1}X^T Y\]

In Octave, it could compute by \lstinline{pinv(x'*t)*x'*y}.

With normal equation, you do not need to choose a \(\alpha\) and to iterate. BUT, when the dimesion is large, the computation \((X^T X)^{-1}\) could be very slow (it's a \(O(n^3)\) algorithm). With gradient descent, it could work well even dimesion is large.

\subsection{Normal Equation Noninvertibility}

What will happen if \(X^T X\) is not invertible? The pseudo inverse is used when \begin{itemize}
    \item redundant features / linearly dependent 
    \item too many features 
\end{itemize}

\section{Octave Tutorial}

Octave is a good language for algorithm prototyping. 

\subsection{Basic Operations}

In fact, it's just so similar to MATLAB. For example, the \lstinline{ones, zeros, eye, rand}, and the vector form.

\begin{ocode}
    w = rand(1, 10000) + (-6) % uniform distribution
    hist(w, 100)
\end{ocode} 


\begin{ocode}
    w = randn(1, 10000) + (-6) % normal distribution
    hist(w, 100)
\end{ocode} 

\subsection{Moving Data Around}

It's about how to move data into Octave for certain tasks. 

For data file, use the \lstinline{load} and \lstinline{save}. For matrix, use the column and row index with ":", like \lstinline{vec(col1:col2)} and matrix concat could be rather easy, like \lstinline{c = [a b]}.

\subsection{Computing on Data}

For coresponding or elemenet-wise operations, there is a dot sign befor the opration, like \lstinline{c = a .* b}, where a and b get the same size. Without the dot sign, it will be a matrix multiplication. And transpose is important to matrix which is bind to quatation mark, like \lstinline{AT = A'}.

For most functions, they could be applied to vector or matrix form, like \lstinline{abs([1,-2,3])}, \lstinline{a = [1 3 5 -9]; a_b = a > 2}.



Some functions:

\begin{enumerate}
    \item \lstinline{magic}: return a n by n magic square matrix
    \item \lstinline{sum}: return sum of matrix
    \item \lstinline{prod}: return prod of matrix
    \item \lstinline{A(:)}: turn A into a vector
\end{enumerate}

\subsection{Plotting Data}

In implying algorithm, plot is a very important tools. Just like in MATLAB, use \lstinline{vectors, plot, figure, subplot, imagesc}. Some controll command like \lstinline{colorbar, colormap, hold on, xlable, ylable, legend, title,}

\subsection{Control Statements and Functions}

In most programming languages, there are \lstinline{for, while if} statements for controll.

Besides, we can define our own functions in seperate files and use them by \lstinline{addpath} or \lstinline{cd}

\subsection{Vectorization}

With vectorization, code can be more efficient and quick to imply.

For example, we can use vector transpose to compute the inner product rather than for-loop.



\section*{Words}

prototyping 原型设计

clunky 笨重的

% End Here

\let\chapname\undefined
\ifx\mainclass\undefined
\end{document}
\fi 