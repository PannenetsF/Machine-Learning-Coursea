\ifx\mainclass\undefined
\documentclass[en,11pt,english,black,simple]{../elegantbook}
\input{../needed.tex}
\begin{document}
\fi 
\def\chapname{07optimibj}

% Start Here
\chapter{Optimization Objective}

\section{Optimization Objective}

We will introduce supported vector machine, which is supervised. 

For a sigmoid function, if the y = 1, we want the \(h_\theta(x)\approx 1\) and \(\theta^T x \gg 0\). 

Support vector machine simplifies the cost function of logistic regression into a straight line. 

\[J(\theta) = \frac{1}{m}\left[\sum_{i = 1}^{m} y^{(i)} \text{cost}_1 (\theta^T x^{(i)}) + (1 - y^{(i)}) \text{cost}_0 (\theta^T x^{(i)})\right] + \frac{\lambda}{2 m} \sum_{i = 0}^n \theta_i^2\]

As the SVM is two order, the co-efficient before the expression could be dismissed. 

The hypothesis will be this: 

\[h_\theta(x) = 1 \text{ if } \theta^T x \leq 0; 0, \text{ else}\]

\section{Large Margin Intuition}

SVM is called large margin intuition sometimes. The threshold 1 and -1 make the SVM "safer". 

SVM would give a decision boundary that seperate the region with a margin. A large co-efficient will turn the SVM to a sensitive one.

The SVM decision boundary is decided by inner product.

\section{Kernels}

If we need a non-linear decision boundary, we can apply some landmarks and compute new features depending on proximity to landmarks.

The similarity could be expressed as:

\[\text{similarity}(x, l^{(i)}) = \exp(-\frac{||x-l^{(i)}||^2}{2 \sigma^2s})\] 

It's called Guass Kernel, for it's based on Guass Distribution. The problem is how to get the landmarks.

But how to choose the landmarks? We can set the training examples as landmarks. With all the landmarks we can get a feature vector contains the similarity of each landmarks. 

So for a SVM with kernels we can predict \(y = 1\) if \(\theta^T f \ge 0\). The training is to :(ingore the \(\theta_0\) for the squared items)
\[\min \left[C \sum_{i = 1}^m y^{(i)} cost_1 (\theta^T f^{(i)}) + (1-y^{(i)}) cost_0 (\theta^T f^{(i)}) + \frac{1}{2}\sum_{j=1}^{m}\theta_j^2\right]\]

We need to apply the scaling to avoid a very BIG distance in the Guassian Kernel.

\section{SVM and Logistic regression}

If number of features is large, use logistic regression or SVM with linear kernel.

If n is small, m is intermediate, use SVM with Guassian Kernel.

If n is small, m is large, add some features then use logistic regression or SVM with linear kernel.
% \section{Multi-Class SVM}


% End Here

\let\chapname\undefined
\ifx\mainclass\undefined
\end{document}
\fi 