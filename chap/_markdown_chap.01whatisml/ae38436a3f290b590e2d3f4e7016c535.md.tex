In a univariate problems, we have:\markdownRendererInterblockSeparator
{}[ h\markdownRendererEmphasis{\theta(x) = \theta}0 + \theta_1 x ]\markdownRendererInterblockSeparator
{}And the (\theta_i) is so called \textbf{Parameters}.\markdownRendererInterblockSeparator
{}Regression is to minimize the difference between (h(x)) and (y). The cost function defined as:\markdownRendererInterblockSeparator
{}[ J (\theta\markdownRendererEmphasis{0, \theta}1) = \frac{1}{2 m} \sum\markdownRendererEmphasis{{i = 0}^{m} \left(h}\theta(x^{(i)} - y^{(i)})^2\right) ]\markdownRendererInterblockSeparator
{}Of course, for each parameters's vector we can compute its (J) value. Then we can get the global minimum to get the best fit curve. Later, we will learn an efficient algorithm to find the minimum cost.\markdownRendererInterblockSeparator
{}\markdownRendererHeadingTwo{Parameter Learning}\markdownRendererInterblockSeparator
{}\markdownRendererHeadingTwo{Linear Algebra Review}\relax